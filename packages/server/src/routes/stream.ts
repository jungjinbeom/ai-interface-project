import { FastifyInstance } from 'fastify';
import { v4 as uuidv4 } from 'uuid';
import { ChatCompletionRequest, ChatMessage, ChatStreamChunk } from 'shared/types/chat';
import { fallbackService, openaiService } from '@/services';
import { threadManager } from '@/services/threadManager';
import OpenAI from 'openai';

export function registerStreamRoutes(fastify: FastifyInstance) {
    // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë¼ìš°íŠ¸
    fastify.post<{ Body: ChatCompletionRequest }>('/api/chat/stream', async (request, reply) => {
        try {
            const { messages, conversationId } = request.body;

            const messageId = uuidv4();
            const convoId = conversationId || uuidv4();

            // Get or create thread
            const thread = threadManager.getOrCreateThread(convoId, 'New Chat');

            // Save any new user messages that aren't in the thread yet
            const lastUserMessage = messages[messages.length - 1];
            if (lastUserMessage && lastUserMessage.role === 'user') {
                // Check if this message is already in the thread
                const existingMessage = thread.messages.find(
                    (m) => m.content === lastUserMessage.content && m.role === 'user'
                );
                if (!existingMessage) {
                    const userMessage: ChatMessage = {
                        id: uuidv4(),
                        role: lastUserMessage.role,
                        content: lastUserMessage.content,
                        createdAt: new Date().toISOString(),
                    };
                    threadManager.addMessageToThread(convoId, userMessage);
                }
            }

            let assistantResponse = '';

            // ì‘ë‹µ í—¤ë” ì„¤ì • (SSE í˜•ì‹)
            reply.raw.writeHead(200, {
                'Content-Type': 'text/event-stream',
                'Cache-Control': 'no-cache',
                Connection: 'keep-alive',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type, Authorization',
            });

            // OpenAIê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if (!openaiService.isInitialized()) {
                // Fallback ì„œë¹„ìŠ¤ ì‚¬ìš©
                const fallbackMessages = messages.map((msg) => ({
                    role: msg.role as 'user' | 'assistant' | 'system',
                    content: msg.content,
                }));

                // ì´ˆê¸° ê²½ê³  ë©”ì‹œì§€
                const warningMessage = 'âš ï¸ OpenAI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n';
                assistantResponse += warningMessage;
                for (const char of warningMessage) {
                    const chunk: ChatStreamChunk = {
                        id: messageId,
                        content: char,
                        role: 'assistant',
                        conversationId: convoId,
                        isDone: false,
                    };
                    reply.raw.write(`data: ${JSON.stringify(chunk)}\n\n`);
                    await new Promise((resolve) => setTimeout(resolve, 30));
                }

                // Fallback ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                for await (const mockChunk of fallbackService.createMockStreamingCompletion(fallbackMessages)) {
                    assistantResponse += mockChunk.content;
                    const streamChunk: ChatStreamChunk = {
                        id: messageId,
                        content: mockChunk.content,
                        role: 'assistant',
                        conversationId: convoId,
                        isDone: false,
                    };
                    reply.raw.write(`data: ${JSON.stringify(streamChunk)}\n\n`);
                }

                // ì„¤ì • ì•ˆë‚´ ë©”ì‹œì§€
                const setupMessage =
                    '\n\nğŸ’¡ ì‹¤ì œ AI ì‘ë‹µì„ ë°›ìœ¼ë ¤ë©´:\n1. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”\n2. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”';
                assistantResponse += setupMessage;
                for (const char of setupMessage) {
                    const chunk: ChatStreamChunk = {
                        id: messageId,
                        content: char,
                        role: 'assistant',
                        conversationId: convoId,
                        isDone: false,
                    };
                    reply.raw.write(`data: ${JSON.stringify(chunk)}\n\n`);
                    await new Promise((resolve) => setTimeout(resolve, 30));
                }

                // ì™„ë£Œ ì²­í¬
                const finalChunk: ChatStreamChunk = {
                    id: messageId,
                    content: '',
                    role: 'assistant',
                    conversationId: convoId,
                    isDone: true,
                };
                reply.raw.write(`data: ${JSON.stringify(finalChunk)}\n\n`);
            } else {
                // OpenAI ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = messages.map((msg) => ({
                    role: msg.role as 'user' | 'assistant' | 'system',
                    content: msg.content,
                }));

                // OpenAI ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
                const stream = await openaiService.createStreamingChatCompletion(openaiMessages);

                // OpenAI ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
                for await (const chunk of stream) {
                    const content = chunk.choices[0]?.delta?.content || '';
                    const finishReason = chunk.choices[0]?.finish_reason;

                    if (content) {
                        assistantResponse += content;
                        const streamChunk: ChatStreamChunk = {
                            id: messageId,
                            content,
                            role: 'assistant',
                            conversationId: convoId,
                            isDone: finishReason === 'stop',
                        };

                        // SSE í˜•ì‹ìœ¼ë¡œ ê° ì²­í¬ ì „ì†¡
                        reply.raw.write(`data: ${JSON.stringify(streamChunk)}\n\n`);
                    }

                    // ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹œ ì¢…ë£Œ
                    if (finishReason === 'stop') {
                        break;
                    }
                }
            }

            // Save assistant response to thread
            if (assistantResponse.trim()) {
                const assistantMessage: ChatMessage = {
                    id: messageId,
                    role: 'assistant',
                    content: assistantResponse,
                    createdAt: new Date().toISOString(),
                };
                threadManager.addMessageToThread(convoId, assistantMessage);
            }

            // SSE ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹ í˜¸
            reply.raw.write('data: [DONE]\n\n');
            reply.raw.end();
        } catch (err) {
            fastify.log.error(err);
            reply.code(500).send({ error: 'ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' });
        }
    });
}
