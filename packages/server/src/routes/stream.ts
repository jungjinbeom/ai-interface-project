import { FastifyInstance } from 'fastify';
import { v4 as uuidv4 } from 'uuid';
import { ChatCompletionRequest, ChatStreamChunk } from 'shared/types/chat';
import { fallbackService, openaiService } from '@/services';
import OpenAI from 'openai';

export function registerStreamRoutes(fastify: FastifyInstance) {
    // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ë¼ìš°íŠ¸
    fastify.post<{ Body: ChatCompletionRequest }>('/api/chat/stream', async (request, reply) => {
        try {
            const { messages, conversationId } = request.body;

            const messageId = uuidv4();
            const convoId = conversationId || uuidv4();

            // ì‘ë‹µ í—¤ë” ì„¤ì •
            reply.raw.writeHead(200, {
                'Content-Type': 'application/json',
                'Transfer-Encoding': 'chunked',
            });

            // OpenAIê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if (!openaiService.isInitialized()) {
                // Fallback ì„œë¹„ìŠ¤ ì‚¬ìš©
                const fallbackMessages = messages.map((msg) => ({
                    role: msg.role as 'user' | 'assistant' | 'system',
                    content: msg.content,
                }));

                // ì´ˆê¸° ê²½ê³  ë©”ì‹œì§€
                const warningMessage = 'âš ï¸ OpenAI APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n';
                for (const char of warningMessage) {
                    const chunk: ChatStreamChunk = {
                        id: messageId,
                        content: char,
                        role: 'assistant',
                        conversationId: convoId,
                        isDone: false,
                    };
                    reply.raw.write(JSON.stringify(chunk) + '\n');
                    await new Promise((resolve) => setTimeout(resolve, 30));
                }

                // Fallback ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
                for await (const mockChunk of fallbackService.createMockStreamingCompletion(fallbackMessages)) {
                    const streamChunk: ChatStreamChunk = {
                        id: messageId,
                        content: mockChunk.content,
                        role: 'assistant',
                        conversationId: convoId,
                        isDone: false,
                    };
                    reply.raw.write(JSON.stringify(streamChunk) + '\n');
                }

                // ì„¤ì • ì•ˆë‚´ ë©”ì‹œì§€
                const setupMessage =
                    '\n\nğŸ’¡ ì‹¤ì œ AI ì‘ë‹µì„ ë°›ìœ¼ë ¤ë©´:\n1. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”\n2. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”';
                for (const char of setupMessage) {
                    const chunk: ChatStreamChunk = {
                        id: messageId,
                        content: char,
                        role: 'assistant',
                        conversationId: convoId,
                        isDone: false,
                    };
                    reply.raw.write(JSON.stringify(chunk) + '\n');
                    await new Promise((resolve) => setTimeout(resolve, 30));
                }

                // ì™„ë£Œ ì²­í¬
                const finalChunk: ChatStreamChunk = {
                    id: messageId,
                    content: '',
                    role: 'assistant',
                    conversationId: convoId,
                    isDone: true,
                };
                reply.raw.write(JSON.stringify(finalChunk) + '\n');
            } else {
                // OpenAI ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                const openaiMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = messages.map((msg) => ({
                    role: msg.role as 'user' | 'assistant' | 'system',
                    content: msg.content,
                }));

                // OpenAI ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
                const stream = await openaiService.createStreamingChatCompletion(openaiMessages);

                // OpenAI ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
                for await (const chunk of stream) {
                    const content = chunk.choices[0]?.delta?.content || '';
                    const finishReason = chunk.choices[0]?.finish_reason;

                    if (content) {
                        const streamChunk: ChatStreamChunk = {
                            id: messageId,
                            content,
                            role: 'assistant',
                            conversationId: convoId,
                            isDone: finishReason === 'stop',
                        };

                        // JSON í˜•ì‹ìœ¼ë¡œ ê° ì²­í¬ ì „ì†¡
                        reply.raw.write(JSON.stringify(streamChunk) + '\n');
                    }

                    // ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì‹œ ì¢…ë£Œ
                    if (finishReason === 'stop') {
                        break;
                    }
                }
            }

            reply.raw.end();
        } catch (err) {
            fastify.log.error(err);
            reply.code(500).send({ error: 'ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' });
        }
    });
}
